{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fb1bbd",
   "metadata": {},
   "source": [
    "# Demystifying N-Grams in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a36df",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Natural language processing stands at the fascinating intersection of computer science and linguistics, enabling machines to understand and interpret human language. A fundamental concept in NLP is the n-gram, a contiguous sequence of n items from a given sample of text or speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640b041",
   "metadata": {},
   "source": [
    "### Understanding N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e625db",
   "metadata": {},
   "source": [
    "N-grams are the building blocks of text and speech-based data. They are used to model language based on the prediction of words, given the words that precede them. An n-gram can be a single word (unigram), a pair of consecutive words (bigram), or a sequence of words (trigram and beyond). The choice of n affects the model's performance and its understanding of context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81398e9c",
   "metadata": {},
   "source": [
    "### Why N-Grams?\n",
    "* Simplicity and Efficiency: N-grams are straightforward to implement and can effectively capture the local context within text data.\n",
    "* Versatility: They are used in various NLP tasks, including text classification, sentiment analysis, and machine translation.\n",
    "* Predictive Modeling: N-grams can predict the likelihood of the next item in a sequence, making them essential for language modeling and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fd788",
   "metadata": {},
   "source": [
    "### Implementing an N-Gram Language Model in Python\n",
    "To illustrate the concept of n-grams, let's dive into a simple Python implementation of an N-Gram Language Model. This model will be capable of generating new text based on a given seed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff54cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05b33251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngrams = {}\n",
    "        self.start_tokens = ['<start>'] * (n - 1)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        for sentence in corpus:\n",
    "            tokens = self.start_tokens + sentence.split() + ['<end>']\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                if ngram in self.ngrams:\n",
    "                    self.ngrams[ngram] += 1\n",
    "                else:\n",
    "                    self.ngrams[ngram] = 1\n",
    "\n",
    "    def generate_text(self, seed_text, length=10):\n",
    "        seed_tokens = seed_text.split()\n",
    "        padded_seed_text = self.start_tokens[-(self.n - 1 - len(seed_tokens)):] + seed_tokens\n",
    "        generated_text = list(padded_seed_text)\n",
    "        current_ngram = tuple(generated_text[-self.n + 1:])\n",
    "\n",
    "        for _ in range(length):\n",
    "            next_words = [ngram[-1] for ngram in self.ngrams.keys() if ngram[:-1] == current_ngram]\n",
    "            if next_words:\n",
    "                next_word = random.choice(next_words)\n",
    "                generated_text.append(next_word)\n",
    "                current_ngram = tuple(generated_text[-self.n + 1:])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return ' '.join(generated_text[len(self.start_tokens):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bdb1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngrams = {}\n",
    "        # Start and end tokens signify the beginning and conclusion of a sentence to a model.\n",
    "        self.start_tokens = ['<start>'] * (n - 1)\n",
    "    \"\"\"\n",
    "    We define a method named train to train the language model on a given corpus. \n",
    "    Then, we iterate through each sentence in the provided corpus. \n",
    "    We tokenize the sentence by adding start tokens, splitting it into individual words, \n",
    "    and appending an end token. Moreover, we iterate through the sentence to create n-grams \n",
    "    by considering sequences of length n. We extract the current n-gram as a tuple from \n",
    "    the token sequence and update the frequency count of the current n-gram in the ngrams dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, corpus):\n",
    "        for sentence in corpus:\n",
    "            tokens = self.start_tokens + sentence.split() + ['<end>']\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                if ngram in self.ngrams:\n",
    "                    self.ngrams[ngram] += 1\n",
    "                else:\n",
    "                    self.ngrams[ngram] = 1\n",
    "            #print(self.ngrams,\"\\n\")\n",
    "            \n",
    "            \n",
    "    #generates text based on the trained language model, starting with a seed text.\n",
    "    def generate_text(self, seed_text, length=10):\n",
    "        seed_tokens = seed_text.split()\n",
    "        padded_seed_text = self.start_tokens[-(self.n - 1 - len(seed_tokens)):] + seed_tokens\n",
    "        #print(padded_seed_text)\n",
    "        generated_text = list(padded_seed_text)\n",
    "        #print(generated_text)\n",
    "        current_ngram = tuple(generated_text[-self.n + 1:])\n",
    "        #print(current_ngram)\n",
    "\n",
    "        for _ in range(length):\n",
    "            next_words = [ngram[-1] for ngram in self.ngrams.keys() if ngram[:-1] == current_ngram]\n",
    "            if next_words:\n",
    "                #print(next_words)\n",
    "                next_word = random.choice(next_words)\n",
    "                generated_text.append(next_word)\n",
    "                #print(generated_text)\n",
    "                current_ngram = tuple(generated_text[-self.n + 1:])\n",
    "                #print(current_ngram)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return ' '.join(generated_text[len(self.start_tokens):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9ce78",
   "metadata": {},
   "source": [
    "<b>NGramLanguageModel</b> class can be trained on a corpus of text and then used to generate new text sequences based on a seed text. It illustrates how n-grams capture the context of a given piece of text to predict subsequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f60e3",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "* Initialization: The model is initialized with a specific n-gram size (n).\n",
    "* Training: It takes a corpus of sentences, splits them into tokens, and builds a dictionary of n-grams with their occurrence counts.\n",
    "* Text Generation: Given a seed text, the model generates text by predicting the next word based on the current n-gram context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c74cb987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: This\n",
      "Generated text: is a simple\n"
     ]
    }
   ],
   "source": [
    "# Toy corpus\n",
    "toy_corpus = [\n",
    "    \"This is a simple example.\",\n",
    "    \"The example demonstrates an N-gram language model.\",\n",
    "    \"N-grams are used in natural language processing.\",\n",
    "    \"This is a toy corpus for language modeling.\"\n",
    "]\n",
    "\n",
    "# Change n-gram order here\n",
    "n = 3 \n",
    "model = NGramLanguageModel(n)  \n",
    "model.train(toy_corpus)\n",
    "\n",
    "# seed text 1\n",
    "seed_text = \"This\"  \n",
    "generated_text = model.generate_text(seed_text, length=3)\n",
    "print(\"Seed text:\", seed_text)\n",
    "print(\"Generated text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c86823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: This\n",
      "Generated text: is a simple example.\n"
     ]
    }
   ],
   "source": [
    "# Seed Text 2\n",
    "seed_text = \"This\"  \n",
    "generated_text = model.generate_text(seed_text, length=4)\n",
    "print(\"Seed text:\", seed_text)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0f90e",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "* Limited Context: N-grams can only capture limited context, often missing longer dependencies in text that are crucial for understanding sentence structure and meaning.\n",
    "* Sparsity Problem: As the value of n increases, the model becomes sparse, leading to a dramatic increase in the number of possible n-grams, many of which may never appear in the training data.\n",
    "* Overfitting to Training Data: N-gram models can overfit to the training data, making them less effective at generalizing to unseen text or capturing the variability in language.\n",
    "* Inability to Handle Out-of-Vocabulary Words: N-gram models struggle with words not seen during training, limiting their ability to deal with new or rare words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
